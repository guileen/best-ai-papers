**2012年：**
- ["ImageNet Classification with Deep Convolutional Neural Networks"](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) - Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton

**2013年：**
- ["Playing Atari with Deep Reinforcement Learning"](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) - Volodymyr Mnih et al.

**2014年：**
- ["Sequence to Sequence Learning with Neural Networks"](https://arxiv.org/abs/1409.3215) - Ilya Sutskever, Oriol Vinyals, Quoc V. Le
- ["Dropout: A Simple Way to Prevent Neural Networks from Overfitting"](http://www.jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf) - Nitish Srivastava et al.
- ["Neural Machine Translation by Jointly Learning to Align and Translate"](https://arxiv.org/abs/1409.0473) - Dzmitry Bahdanau et al.

**2015年：**
- ["Deep Residual Learning for Image Recognition"](https://arxiv.org/abs/1512.03385) - Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
- ["Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"](https://arxiv.org/abs/1502.03044) - Kelvin Xu et al.
- ["Human-level control through deep reinforcement learning"](https://www.nature.com/articles/nature14236) - Volodymyr Mnih et al.

**2016年：**
- ["Attention is All You Need"](https://arxiv.org/abs/1706.03762) - Ashish Vaswani et al. (Transformer)

**2017年：**
- ["BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"](https://arxiv.org/abs/1810.04805) - Jacob Devlin et al. (自然语言处理)

**2018年：**
- ["A Style-Based Generator Architecture for Generative Adversarial Networks"](https://arxiv.org/abs/1812.04948) - Tero Karras et al. (GANs)

**2019年：**
- ["XLNet: Generalized Autoregressive Pretraining for Language Understanding"](https://arxiv.org/abs/1906.08237) - Yang et al. (自然语言处理)

**2020年：**
- ["CLIP: Connecting Text and Images for Zero-Shot Learning"](https://openai.com/research/clip) - OpenAI (视觉与自然语言处理交叉)
