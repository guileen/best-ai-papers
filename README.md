# Best-AI-Papers: A Curated List of Foundational and Transformative AI Research

## Introduction

This repository aims to provide a curated list of foundational and transformative papers in the field of Artificial Intelligence (AI). These papers have been instrumental in shaping the AI landscape and continue to hold significant relevance in today's era of large-scale models and the move towards Artificial General Intelligence (AGI). This list can serve as a starting point for those interested in understanding the key milestones in AI, as well as a reference for researchers and professionals.

## Reading Suggestions
- ðŸ“• **Essential Reading**: Papers that anyone entering the field of AI should read.
- ðŸŽ¯ **Deep Dive**: For readers looking for a more in-depth understanding.
- ðŸ’¡ **Innovative Ideas**: Cutting-edge papers with novel approaches or ideas.
- ðŸ•’ **Quick Overview**: Brief, yet impactful papers that can be read quickly.
- ðŸ“œ **Historical Context**: Papers that have been influential in the development of AI and continue to be of historical importance.

---

# Pre-Deep Learning Milestones in Machine Learning and AI

This section lists foundational papers that made significant contributions to the field of Artificial Intelligence and Machine Learning before the resurgence of deep learning. These papers laid the groundwork for many modern-day algorithms and concepts, and their impact extends beyond their original time of publication.

- ðŸ“• 1950: [Computing Machinery and Intelligence](https://doi.org/10.1093/mind/LIX.236.433) by Alan Turing
- ðŸ“• 1958: [Perceptron: A probabilistic model for information storage and organization in the brain](https://psycnet.apa.org/record/1959-09865-001) by Frank Rosenblatt
- ðŸ“• 1985: [Learning Internal Representations by Error Propagation (Boltzmann Machine)](https://www.cs.toronto.edu/~hinton/absps/pdp8.pdf) by Hinton et al.
- ðŸŽ¯ 1986: [Learning Representations by Back-propagating Errors (Backpropagation)](https://www.nature.com/articles/323533a0) by Geoffrey Hinton, David Rumelhart, Ronald Williams
- ðŸ“œ 1986: [Induction of Decision Trees (CART)](https://link.springer.com/article/10.1007/BF00116251) by Breiman et al.
- ðŸ•’ 1989: [A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition (HMM)](https://ieeexplore.ieee.org/abstract/document/18626) by Rabiner
- ðŸ“• 1989: [Multilayer Feedforward Networks are Universal Approximators](https://www.sciencedirect.com/science/article/abs/pii/0893608089900208) by Hornik et al.
- ðŸŽ¯ 1992: [A Training Algorithm for Optimal Margin Classifiers (SVM)](https://dl.acm.org/doi/10.1145/130385.130401) by Boser et al.
- ðŸ’¡ 1996: [Bagging Predictors](https://link.springer.com/article/10.1007/BF00058655) by Breiman
- ðŸ•’ 1998: [Gradient-Based Learning Applied to Document Recognition (CNN/GTN)](https://ieeexplore.ieee.org/abstract/document/726791) by Lecun et al.
- ðŸ“œ 2001: [Random Forests](https://link.springer.com/article/10.1023/a:1010933404324) by Breiman
- ðŸŽ¯ 2001: [A Fast and Elitist Multiobjective Genetic Algorithm (NSGA-II)](https://ieeexplore.ieee.org/abstract/document/996017) by Deb et al.
- ðŸ’¡ 2003: [Latent Dirichlet Allocation (LDA)](https://jmlr.csail.mit.edu/papers/v3/blei03a.html) by Blei et al.
- ðŸ“• 2006: [Reducing the Dimensionality of Data with Neural Networks (Autoencoder)](https://www.science.org/doi/abs/10.1126/science.1127647) by Hinton and Salakhutdinov
- ðŸŽ¯ 2006: [A Fast Learning Algorithm for Deep Belief Nets](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf) by Geoffrey Hinton, Simon Osindero, Yee-Whye Teh
- ðŸŽ¯ 2008: [Visualizing Data using t-SNE (t-SNE)](https://www.jmlr.org/papers/v9/vandermaaten08a.html) by van der Maaten and Hinton
- ðŸ“œ 2009: [ImageNet: A Large-Scale Hierarchical Image Database (ImageNet)](https://ieeexplore.ieee.org/document/5206848) by Deng et al.

### Architectures and Algorithms

- ðŸ“• 2012: [ImageNet Classification with Deep Convolutional Neural Networks (AlexNet)](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)
- ðŸŽ¯ 2013: [Efficient Estimation of Word Representations in Vector Space (Word2vec)](https://arxiv.org/abs/1301.3781)
- ðŸ’¡ 2013: [Auto-Encoding Variational Bayes (VAE)](https://arxiv.org/abs/1312.6114)
- ðŸ“• 2014: [Generative Adversarial Networks (GAN)](https://papers.nips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html)
- ðŸŽ¯ 2014: [Adam: A Method for Stochastic Optimization (Adam)](https://arxiv.org/abs/1412.6980)
- ðŸ•’ 2014: [Neural Machine Translation by Jointly Learning to Align and Translate (Seq2Seq)](https://arxiv.org/abs/1409.0473)
- ðŸ“• 2014: [Dropout: A Simple Way to Prevent Neural Networks from Overfitting (Dropout)](https://jmlr.org/papers/v15/srivastava14a.html)
- ðŸ’¡ 2015: [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (BatchNorm)](http://proceedings.mlr.press/v37/ioffe15.html)
- ðŸ“• 2015: [Deep Residual Learning for Image Recognition (ResNet)](https://arxiv.org/abs/1512.03385)
- ðŸŽ¯ 2016: [You Only Look Once: Unified, Real-Time Object Detection (YOLO)](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html)
- ðŸ•’ 2016: [WaveNet: A Generative Model for Raw Audio](https://arxiv.org/abs/1609.03499)
- ðŸ’¡ 2017: [Attention is All you Need (Transformer)](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)
- ðŸ“œ 2017: [Dynamic Routing Between Capsules (Capsule Networks)](https://arxiv.org/abs/1710.09829)
- ðŸ•’ 2018: [Glow: Generative Flow with Invertible 1x1 Convolutions](https://arxiv.org/abs/1807.03039)
- ðŸ“• 2018: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (BERT)](https://arxiv.org/abs/1810.04805)
- ðŸ•’ 2019: [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
- ðŸŽ¯ 2020: [Language Models are Few-Shot Learners (GPT-3)](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)
- ðŸ•’ 2020: [T5: Text-To-Text Transfer Transformer](https://arxiv.org/abs/1910.10683)
- ðŸ•’ 2021: [CLIP: Connecting Texts and Images using Contrastive Learning](https://arxiv.org/abs/2103.00020)
- ðŸ’¡ 2021: [Highly accurate protein structure prediction with AlphaFold (Alphafold)](https://www.nature.com/articles/s41586-021-03819-2)
- ðŸŽ¯ 2021: [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) by OpenAI
- ðŸŽ¯ 2022: [High-Resolution Image Synthesis with Latent Diffusion Models(Stable Diffussion)](https://ommer-lab.com/research/latent-diffusion-models/)

## Future Work

As the field of artificial intelligence continues to evolve, this README will strive to keep up-to-date with the latest groundbreaking research papers and methodologies. Contributions are welcome.
